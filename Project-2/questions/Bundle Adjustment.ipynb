{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bundle Adjustment\n",
    "\n",
    "Part of this assignment is based on scipy-cookbook. It will take around 2 hours to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the paper 'Building Rome in a Day' and briefly write about the fundamental idea behind the problem and solution. No need to be verbose, just write about the challenge with the task and how the pipeline is implemented (do not include details about performance/parallelization).\n",
    "\n",
    "2. How is this task different from a SLAM problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "We have a set of points in real world defined by their coordinates $(X, Y, Z)$ in some apriori chosen \"world coordinate frame\". We photograph these points by different cameras, which are characterized by their orientation and translation relative to the world coordinate frame and also by focal length and two radial distortion parameters (9 parameters in total). Then we precicely measure 2-D coordinates $(x, y)$ of the points projected by the cameras on images. Our task is to refine 3-D coordinates of original points as well as camera parameters, by minimizing the sum of squares of reprojecting errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a dataset from http://grail.cs.washington.edu/projects/bal/ for this task. Feel free to choose any of the ones mentioned on the page. Take the smallest file from each dataset (you can choose any but it will take longer to run, consume more memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import copy\n",
    "import bz2\n",
    "import os\n",
    "import numpy as np\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"http://grail.cs.washington.edu/projects/bal/data/\"\n",
    "\n",
    "DATASET_NAME = \"final/\"\n",
    "FILE_NAME = \"problem-13682-4456117-pre.txt.bz2\"\n",
    "\n",
    "URL = BASE_URL + DATASET_NAME + FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(FILE_NAME):\n",
    "    urllib.request.urlretrieve(URL, FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the data from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bal_data(file_name):\n",
    "    with bz2.open(file_name, \"rt\") as file:\n",
    "        n_cameras, n_points, n_observations = map(\n",
    "            int, file.readline().split())\n",
    "\n",
    "        camera_indices = np.empty(n_observations, dtype=int)\n",
    "        point_indices = np.empty(n_observations, dtype=int)\n",
    "        points_2d = np.empty((n_observations, 2))\n",
    "\n",
    "        for i in range(n_observations):\n",
    "            camera_index, point_index, x, y = file.readline().split()\n",
    "            camera_indices[i] = int(camera_index)\n",
    "            point_indices[i] = int(point_index)\n",
    "            points_2d[i] = [float(x), float(y)]\n",
    "\n",
    "        camera_params = np.empty(n_cameras * 9)\n",
    "        for i in range(n_cameras * 9):\n",
    "            camera_params[i] = float(file.readline())\n",
    "        camera_params = camera_params.reshape((n_cameras, -1))\n",
    "\n",
    "        points_3d = np.empty(n_points * 3)\n",
    "        for i in range(n_points * 3):\n",
    "            points_3d[i] = float(file.readline())\n",
    "        points_3d = points_3d.reshape((n_points, -1))\n",
    "\n",
    "    return camera_params, points_3d, camera_indices, point_indices, points_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_params, points_3d, camera_indices, point_indices, points_2d = read_bal_data(FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera_params: (13682, 9);\n",
      "points_3d: (4456117, 3);\n",
      "camera_indices: (28987644,); \n",
      "point_indices: (28987644,); \n",
      "points_2d: (28987644, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"camera_params: {camera_params.shape};\\npoints_3d: {points_3d.shape};\\n\"\n",
    "        f\"camera_indices: {camera_indices.shape}; \\npoint_indices: {point_indices.shape}; \\n\"\n",
    "        f\"points_2d: {points_2d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have numpy arrays:\n",
    "\n",
    "1. `camera_params` with shape `(n_cameras, 9)` contains initial estimates of parameters for all cameras. First 3 components in each row form a **rotation vector**, next 3 components form a translation vector, then a focal distance and two distortion parameters.\n",
    "2. `points_3d` with shape `(n_points, 3)` contains initial estimates of point coordinates in the world frame.\n",
    "3. `points_2d` with shape `(n_observations, 2)` contains measured 2-D coordinates of points projected on images in all the observations.\n",
    "4. `camera_ind` with shape `(n_observations,)` gives the index of the camera (from 0 to `n_cameras - 1`) associated with a particular observation.   \n",
    "5. `point_ind` with shape `(n_observations,)` contains indices of 3D points (from 0 to `n_points - 1`) involved in each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Point Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise `points_3d`. It may not look like 'Venice' or any building as we are working with a small subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Mostly useless\n",
    "# pcd = o3d.geometry.PointCloud()\n",
    "# pcd.points = o3d.utility.Vector3dVector(points_3d)\n",
    "# o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many cameras and 3D points do we have? Calculate the number of parameters to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_cameras: 13682\n",
      "n_points: 4456117\n",
      "Total number of parameters to estimate: 13491489\n",
      "Total number of residuals: 57975288\n"
     ]
    }
   ],
   "source": [
    "n_cameras = camera_params.shape[0]\n",
    "n_points = points_3d.shape[0]\n",
    "m = 2 * points_2d.shape[0]\n",
    "n = 9 * n_cameras + 3 * n_points\n",
    "\n",
    "print(\"n_cameras: {}\".format(n_cameras))\n",
    "print(\"n_points: {}\".format(n_points))\n",
    "print(\"Total number of parameters to estimate: {}\".format(n))\n",
    "print(\"Total number of residuals: {}\".format(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose a relatively small problem to reduce computation time, but scipy's algorithm is capable of solving much larger problems, although required time will grow proportionally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the function which returns a vector of residuals. We use numpy vectorized computations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A short review on Transformations\n",
    "\n",
    "Rodrigues Formula: $$\\mathbf{R}=\\cos \\theta \\mathbf{I}+(1-\\cos \\theta) \\mathbf{n n}^{\\mathrm{T}}+\\sin \\theta \\mathbf{n}^{\\wedge}$$\n",
    "If described by a rotation vector, assuming that the rotation axis is a unit length vector $\\mathbf{n}$ and the angle is $\\theta$, then the vector $\\theta \\mathbf{n}$ can also describe this rotation. Here, rot_vecs = $\\theta \\mathbf{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(points, rot_vecs):\n",
    "    \"\"\"Rotate points by given rotation vectors.\n",
    "    \n",
    "    Rodrigues' rotation formula is used.\n",
    "    \"\"\"\n",
    "    theta = np.linalg.norm(rot_vecs, axis=1)[:, np.newaxis] #np.newaxis converts this into a column vector.\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        v = rot_vecs / theta\n",
    "        v = np.nan_to_num(v)\n",
    "    dot = np.sum(points * v, axis=1)[:, np.newaxis]\n",
    "    cos_theta = np.cos(theta)\n",
    "    sin_theta = np.sin(theta)\n",
    "    \n",
    "    return (cos_theta * points) + ((1 - cos_theta) * v * dot) + (sin_theta * np.cross(v, points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A short review on camera modelling & radial distortion\n",
    "\n",
    "\n",
    "\n",
    "- Each pixel moves radially away from (barrel) or towards (pincushion) the image center (c).\n",
    "- As a function of distance from $c: r_{c}^{2}=x_{c}^{2}+y_{c}^{2}$.\n",
    "- The shift $\\gamma$ can be modelled as: $\\gamma=1+k_{1} r_{c}^{2}+k_{2} r_{c}^{4}$ where ${k}_{1}$ and ${k}_{2}$ are radial distortion parameters.\n",
    "- The modified co-ordinates are:\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\hat{x}_{c}=\\gamma x_{c} \\\\\n",
    "\\hat{y}_{c}=\\gamma y_{c}\n",
    "\\end{array} \n",
    "$$\n",
    "\n",
    "- **This is applied before the focal-length multiplier and center shift are applied**: Meaning before $K$ matrix is even applied. But how do we exactly do that?\n",
    "\n",
    "    $$\\mathbf{K}=\\left[\\begin{array}{ccc}\\alpha_{x} & 0 & x_{0} \\\\0 & \\alpha_{y} & y_{0} \\\\0 & 0 & 1\\end{array}\\right] ; \\qquad      \\lambda {p} = \\mathrm{x} =K[R \\quad t] \\mathrm{X}$$\n",
    "\n",
    "    $$x_{final} = \\gamma \\left(\\frac{f_0X}{Z}+c_x \\right)\n",
    "     \\qquad \\color{red} \\bigotimes \\textbf{wrong}$$\n",
    "\n",
    "    $$x_{final} =  \\left(f_0 \\left(\\gamma\\frac{X}{Z} \\right)+c_x \\right)\n",
    "     \\qquad \\color{surd} \\checkmark \\textbf{correct}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing it up\n",
    "Let $\\pmb{P} = (X, Y, Z)^T$ - a radius-vector of a point, $\\pmb{R}$ - a rotation matrix of a camera, $\\pmb{t}$ - a translation vector of a camera, $f$ - its focal distance, $k_1, k_2$ - its distortion parameters. Then the reprojecting is done as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\pmb{Q} = \\pmb{R} \\pmb{P} + \\pmb{t} \\\\\n",
    "\\pmb{q} = -\\begin{pmatrix} Q_x / Q_z \\\\ Q_y / Q_z \\end{pmatrix} \\\\\n",
    "\\pmb{p} = f (1 + k_1 \\lVert \\pmb{q} \\rVert^2 + k_2 \\lVert \\pmb{q} \\rVert^4) \\pmb{q}\n",
    "\\end{align}\n",
    "The resulting vector $\\pmb{p}=(x, y)^T$ contains image coordinates of the original point.\n",
    "![radial_distortion_1.png](../misc/radial_distortion_1.png) \n",
    "![radial_distortion_2.png](../misc/radial_distortion_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(points, camera_params):\n",
    "    \"\"\"Convert 3-D points to 2-D by projecting onto images.\"\"\"\n",
    "    \n",
    "    projpoi = rotate(points, camera_params[:, :3])\n",
    "    projpoi = projpoi + camera_params[:, 3:6]\n",
    "    projpoi = -1*projpoi[:, :2] / projpoi[:, 2, np.newaxis]\n",
    "    n = np.sum(projpoi**2, axis=1)\n",
    "    r = 1 + camera_params[:, 7] * n + camera_params[:, 8] * n**2\n",
    "    projpoi = projpoi * (r * camera_params[:, 6])[:, np.newaxis]\n",
    "    return projpoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(params, n_cameras, n_points, camera_indices, point_indices, points_2d):\n",
    "    \"\"\"Compute residuals.\n",
    "    \n",
    "    `params` contains camera parameters and 3-D coordinates.\n",
    "    \"\"\"\n",
    "    params = copy.deepcopy(params)\n",
    "    camera_params = params[:n_cameras * 9].reshape((n_cameras, 9))\n",
    "    \n",
    "    points_3d = params[n_cameras * 9:].reshape((n_points, 3))\n",
    "    points_proj = project(points_3d[point_indices], camera_params[camera_indices])\n",
    "    return (points_proj - points_2d).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A short review on Structure from Motion\n",
    "### Residual\n",
    "In our lecture, in the residual vector, we  wrote the elements in order: 11, 12, 13.., 1N, then 21, 22.. and so on till MN. However, notice that it is not the case here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M -> camera, N -> 3D point (in our lectures, NOT in this code)\n",
    "![sfm_residual_1.png](./misc/sfm_residual_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that computing Jacobian of `fun` is cumbersome, thus we will rely on the finite difference approximation. To make this process time feasible we provide Jacobian sparsity structure (i. e. mark elements which are known to be non-zero):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sfm_jac_2.png](./misc/sfm_jac_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the matrix is sparse, we can make use of datastructures that are meant for such a usecase - https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the matrix computation has been given to you, you will have to explain this function later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bundle_adjustment_sparsity(n_cameras, n_points, camera_indices, point_indices):\n",
    "    m = camera_indices.size * 2 \n",
    "    n = n_cameras * 9 + n_points * 3\n",
    "            \n",
    "    A = lil_matrix((m, n), dtype=int)\n",
    "\n",
    "    camera_indices = np.sort(camera_indices)\n",
    "    point_indices = np.sort(point_indices)\n",
    "    \n",
    "    i = np.arange(camera_indices.size)\n",
    "    for s in range(9):\n",
    "        A[2 * i, camera_indices * 9 + s] = 1\n",
    "        A[2 * i + 1, camera_indices * 9 + s] = 1\n",
    "\n",
    "    for s in range(3):\n",
    "        A[2 * i, n_cameras * 9 + point_indices * 3 + s] = 1\n",
    "        A[2 * i + 1, n_cameras * 9 + point_indices * 3 + s] = 1\n",
    "            \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THAT'S IT! Now we are ready to use inbuilt library functions!\n",
    "Now we are ready to run optimization. Let's visualize residuals evaluated with the initial parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.hstack((camera_params.ravel(), points_3d.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = fun(x0, n_cameras, n_points, camera_indices, point_indices, points_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f41de13c190>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = bundle_adjustment_sparsity(n_cameras, n_points, camera_indices, point_indices)\n",
    "print(A.shape, n_cameras, n_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Scipy has existing functions for optimization that we can make use of. Write a sentence about the method that is used for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy.optimize import least_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# So far: method='lm'\n",
    "res = least_squares(fun, x0, jac_sparsity=A, verbose=2, x_scale='jac', ftol=1e-4, method='trf',\n",
    "                    args=(n_cameras, n_points, camera_indices, point_indices, points_2d))\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = res.x\n",
    "\n",
    "new_camera_params = params[:n_cameras * 9].reshape((n_cameras, 9))\n",
    "new_points_3d = params[n_cameras * 9:].reshape((n_points, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Optimised Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(new_points_3d)\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `scaling='jac'` was done to automatically scale the variables and equalize their influence on the cost function (clearly the camera parameters and coordinates of the points are very different entities). This option turned out to be crucial for successfull bundle adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimization took {0:.0f} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot residuals at the found solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(res.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see much better picture of residuals now, with the mean being very close to zero. There are some spikes left. It can be explained by outliers in the data, or, possibly, the algorithm found a local minimum (very good one though) or didn't converged enough. Note that the algorithm worked with Jacobian finite difference aproximate, which can potentially block the progress near the minimum because of insufficient accuracy (but again, computing exact Jacobian for this problem is quite difficult)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 2- Part B: Submission details -\n",
    "You are supposed to gain understanding by playing around with the code above and submit your answers to questions asked below. You shouldn't submit this whole notebook, just copy the following cells (starting next cell up until the end of this notebook) and paste it at the end of your Project 2 notebook (already shared on GitHub classrooms, [link](https://github.com/AryanSakaria/Project_2/blob/main/Project_2.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "## 1. SfM pipeline (`6 mark`)\n",
    "\n",
    "To get the context of below questions, take a look at the code above: The same questions have been asked at different places above as comments in the code.\n",
    "\n",
    "1. `0.5 mark` **Basics** - How do we know this (`camera_ind`) information in practical setting? In other words, how do we know observations in `points_2d` belong to which camera. Explain. \n",
    "    - Ans-1 - Basics: Bundle Adjustment is the backend for SfM (structure from motion). We have algorithms like SIFT or ORB on the frontend side to find corresponding points in two images. After this we can use 8 point algorithm to get the Fundamental Matrix. Knowing the camera intrinsics we can get the Essential Matrix and we decompose it to get R,t and K. After this we triangulate back the points from those two images into the 3D world . Here will we know which image the points being backprojected came from (due to camera_ind). PnP can be used on correspondences between the reconstructed world and the third image to get the transformation to adjust to same scale. We will keep repeating this for all images. Thus, we know which of the points_2d belong to which camera because those points are chosen after running SIFT/ORB or other feature detection algorithms on the image from that camera.\n",
    "    \n",
    "    \n",
    "2. `0.5 mark` **Basics** - How do we know this (`point_ind`) information in practical setting?  In other words, how do we know observations in `points_2d` belong to which 3D point. Explain.\n",
    "    - Ans-2 - Basics: We know that images in different views are taken at different poses. Using this we have SIFT/SURF or ORB to find corresponding points between two images. After this we can use 8 point algorithm to estimate the Fundamental Matrix from which we can get the Essential Matrix and decompose them into R,t and K. Further we can triangulate or backproject points to the world to obtain 3D points. Because of this, we now get our 2D-3D correspondences.\n",
    "    \n",
    "    \n",
    "3. `0.5 mark` **Transformations** - `rotate()` function: Why do we use the rodriquez formula? How is this representation different from the standard 3x3 Rotation matrix, why do we use this instead?\n",
    "    - Ans-3 - Transformations: We use rodriguez formula as this reduces the number of unknows. P has 12 unknowns (11 if normalized). R has 6 unknows and if we use rodrigues formula then the number of unknowns coming from rotation drop to 3. Hence the number of overalls in the P matrix come down to 9 which means lesser points are needed. This representation is different as here we represent our rotation about the vector k, a unit vector describing an axis of rotation about which our vector rotates by an angle θ. Formula:\n",
    "    ![Rodrigues Formula](https://wikimedia.org/api/rest_v1/media/math/render/svg/2d63efa533bdbd776434af1a7af3cdafaff1d578)\n",
    "    \n",
    "    \n",
    "4. `0.5 mark` **Transformations** - `project()` function: In the `project()` function, would it make any difference if I do translate first, then rotate? Why/why not?\n",
    "    - Ans-4 - Transformations: Yes, there may be a difference if we translate first and then rotate. Translation before rotation implies that we're moving to a different origin first and rotating about that origin. Hence the transformation matrix will change. Translating after rotating is moving the rotation center. If the translation vector itself lies in the camera frame itself, it is necessary that the point be first rotated to the camera frame before it may be added to the translation vector. Or else the translation vector and the point would lie in some other frame.\n",
    "        \n",
    "        \n",
    "5. `0.5 mark` **Jacobian** - `bundle_adjustment_sparsity()` function: m above is not \"M*N\" (*2) unlike our lecture notes. Why is that so?\n",
    "    - Ans-5 - Jacobian: Each row of the Jacobian corresponds to a residual, so number of rows is equal to number of rows in residuals.  If we have M images and N points in each image, we have MN 2D points, each giving 2 coordinates, so 2MN residuals for rows of the Jacobian. Here, since m contains the info about all the observations made together (all 2D points), i.e., camera_indices.size 2D points we have a different size for it.\n",
    "    \n",
    "    \n",
    "6. `2 mark` **Jacobian & Parameters** - `bundle_adjustment_sparsity()` function: \n",
    "    1.  Why are we doing `n_cameras * 9` here instead of `n_cameras * 12`? Recollect: Every individual motion Jacobian was (1*)12 in our lecture notes. \n",
    "        - Ans 6.1 - Jacobian & Parameters: In our stereo code, projection matrix is dependent on 12 parameters (size = 3x4). However in the bundle adjustment code instead of having to use the elements of the projection matrix, we are using 3 translational parameters, 3 rotational parameters represented in axis-angle form, and 3 parameters for camera intrinsics, namely the focal length and 2 distortion parameters. So our P has 9 parameters, and hence we are using n_cameras * 9.\n",
    "        \n",
    "    2. Ignoring the scale parameters, what was the number of unknown parameters in our lecture notes in terms of `n_cameras` and `n_points`? What is it here in the code? Is it different? If so, what is and why? [Link of notes](https://www.notion.so/Stereo-Structure-from-Motion-9fdd81e4194f4803ac9ba7552df56470).\n",
    "        - Ans 6.2 - Jacobian & Parameters: In our lecture notes, P had 12 unknowns/parameters but here it has 9 parameters. So in our lectures, number of unknown params was (n_cameras * 12  + n_points * 3), where n_points = (n_images x n_points_per_image)/n_times_every_point_is_seen). In the current implementation though, we are already given n_cameras and n_points, and the number of parameters required is also 9, so the total number of unknowns comes to (n_cameras * 9 + n_points * 3)\n",
    "       \n",
    "            \n",
    "            \n",
    "7. `6 mark` **Sparsity, Residual Vector & Jacobian** - `bundle_adjustment_sparsity()` function: Explain what you understand from above 6 lines of code by coding a simple toy example yourself to illustrate how it is different from what you've learnt in class. ([Coding toy example + elaborating in words]- both are compulsory.) For the toy example, you can take something like 3 points all seen from 3 cameras. (You don't actually have to code much, just need to call the existing function) Write that toy example after this cell\n",
    "    - Ans 6 - Sparsity, Residual Vector & Jacobian: The jacobian is computed w.r.t the 9 parameters of P and 3 for each point. Each of these will correspond to one column of the Jacobian. So as we have n_cameras, there will be `n_cameras*9` columns due to P followed by, for n_points, there are `n_points*3` columns due to the points. Hence, to index the columns of jacobian, we move to the corresponding camera by `camera_ind * 9` and to the corresponding point by `camera_ind * 9 + points_ind * 3`. Once we have reached the corresponding section, we update the for loop updates the individual elements of the projection and 3D world point. Each row of the jacobian will come due to an observed point (we have n_points). As each point will have 2 co-ordinates (x and y), so we have 2 rows for each image point. x co-ordinates will be on the odd rows and y co-oridinates will be on the even rows so we can do 2 * i - 1 and 2 * i indexing for the rows of the jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera index for each image point: [0 0 0 1 1 1 2 2 2]\n",
      "Index for the image in 3D World: [0 1 2 0 1 2 0 1 2]\n",
      "Shape of the jacobian: (18, 36)\n",
      "Jacobian J:\n",
      " [[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "n_points = 3  \n",
    "n_cameras = 3  \n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "camera_indices = np.array([j for j in range(n_cameras) for i in range(n_points) ])\n",
    "print(\"Camera index for each image point:\", camera_indices)\n",
    "\n",
    "point_indices = np.array([np.arange(n_points) for i in range(n_cameras)]).flatten()\n",
    "print(\"Index for the image in 3D World:\", point_indices)\n",
    "\n",
    "A = bundle_adjustment_sparsity(n_cameras, n_points, camera_indices, point_indices).toarray()\n",
    "print(f\"Shape of the jacobian: {A.shape}\")\n",
    "print(\"Jacobian J:\\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the output, the Jacobian has a shape = (18, 36).  \n",
    "* Number of columns = n_cameras * 9 + n_points * 3 = 3 * 9 + 3 * 3 = 36\n",
    "* Number of rows = 2 * number of image points = 2 * 3 * 3 = 18 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initializing R,t and 3D points for SfM given 2 images (`4 mark`)\n",
    "\n",
    "Using OpenCV functions, mention how you would initialize R,t (poses) and 3D points for SfM given 2 images and K matrix. You don't need to implement it, just mention function names with input/output arguments clearly and briefly explain what they do (You don't need to give detailed answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "  \n",
    "First we will do feature matrix to get common features in images. For this we will use `cv2.ORB_create()` from OpenCV and use the `detectAndCompute()` to exract ORB features (keypoints and descriptors) on the image pair.  \n",
    "Pseudo code:\n",
    "```\n",
    "orb = cv2.ORB_create()\n",
    "kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "```   \n",
    "After this, we will have keypoints and descriptors for both the images. However, we don't know the correspondences and for this we will create a Brute-Force Matcher `bf = cv2.BFMatcher()`, and perform a K nearest neighbour matching using `ORB descriptors` and `knnMatch()` function.  \n",
    "Pseudo code:\n",
    "```\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(des1,des2,k=2) Alternately, we can use matches = bf.match(des1,des2).  \n",
    "```\n",
    "After this we will have corresponding matches between the two images. Using these matches, we can find the fundamental matrix using `8-point algorithm` or the `cv2.finfFundamentalMat()` function. Since we know the camera intrinsics K, we using the fundamental matrix $F$ we can get the essential matrix, as $E = K^TFK$. Or else we can directly use `E = cv2.findEssentialMat(p1,p2,K)`.\n",
    "\n",
    "We can get $R$ and $t$ by decomposition of the essential matrix $E$ using `cv2.recoverPose(E,p0,p1,K)` which takes $E$, the matches(p1,p2) and calibration matrix $K$ as input and returns $R$ and $t$ as 1st and 2nd index of the output array.\n",
    "  \n",
    "Using this information so far, we can use the 2D correspondences (matches) and the above vectors and matrices to perform triangulation to get the 3D locations for SfM. This is to be achieved using points_3d = cv2.triangulatePoints([p1,p2],output). This gives the 3D points as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
