{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff8b80b",
   "metadata": {},
   "source": [
    "# Assignment 3: ICP + Non-linear least squares optimization\n",
    "  \n",
    "  \n",
    "Team Name: Bhagwaan Bharose  \n",
    "Team ID: Team 11  \n",
    "Member Names: Tushar Choudhary (2019111019), Ayush Goyal (2019111026)\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* You are not allowed to use any external libraries (other than ones being imported below).\n",
    "* The deadline for this assignment is **15-09-21** at 11:55pm.\n",
    "* Plagiarism is **strictly prohibited**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ec2ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af075e46",
   "metadata": {},
   "source": [
    "# Non Linear Least Squares Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b067c6",
   "metadata": {},
   "source": [
    "## 1.1 Gradient Descent\n",
    "Implement the gradient descent algorithm using numpy and what you have learned from class to solve for the parameters of a gaussian distribution.\n",
    "To understand the task in more detail and look at a worked through example, checkout the subsequent section. You have to implement the same using just numpy functions. You can refer to [Shubodh's notes](https://www.notion.so/saishubodh/From-linear-algebra-to-non-linear-weighted-least-squares-optimization-13cf17d318be4d45bb8577c4d3ea4a02) on the same to get a better grasp of the concept before implementing it.\n",
    "* Experiment with the number of iterations.\n",
    "* Experiment with the learning rate.\n",
    "* Experiment with the tolerance.\n",
    "\n",
    "Display your results using matplotlib by plotting graphs for \n",
    "* The cost function value vs the number of iterations\n",
    "* The Ground Truth data values and the predicted data values.\n",
    "\n",
    "Your plots are expected to contain information similar to the plot below:\n",
    "\n",
    "<!-- <figure> -->\n",
    "<img src='./helpers/sample_plt.png' alt=drawing width=500 height=600>\n",
    "\n",
    "<!-- <figcaption align='center'><b>A sample plot, you can use your own plotting template</b></figcaption>\n",
    "</figure> -->\n",
    "<!-- head over to [this page](https://saishubodh.notion.site/Non-Linear-Least-Squares-Solved-example-Computing-Jacobian-for-a-Gaussian-Gradient-Descent-7fd11ebfee034f8ca89cc78c8f1d24d9) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b0c194",
   "metadata": {},
   "source": [
    "## Worked out Example using Gradient Descent\n",
    "\n",
    "A Gaussian distribution parametrized by $a,m,s$ is given by:\n",
    "\n",
    "$$ y(x;a,m,s)=a \\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right) \\tag{1}$$\n",
    "\n",
    "### Jacobian of Gaussian\n",
    "\n",
    "$$\\mathbf{J}_y=\\left[\\frac{\\partial y}{\\partial a} \\quad \\frac{\\partial y}{\\partial m} \\quad \\frac{\\partial y}{\\partial s}\\right] \\\\\n",
    "= \\left[ \\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right); \\frac{a (x-m)}{s^2} \\exp\\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right);  \\frac{a (x-m)^2}{s^3}\\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right)\\right]$$\n",
    "\n",
    "## Problem at hand\n",
    "\n",
    "> Given a set of observations $y_{obs}$ and $x_{obs}$ we want to find the optimum parameters $a,m,s$ which best fit our observations given an initial estimate.\n",
    "\n",
    "Our observations would generally be erroneous and given to us, but for the sake of knowing how good our model is performing, let us generate the observations ourselves by assuming the actual \"actual\" parameter values as $a_{gt}=10; m_{gt} =0; s_{gt} =20$ ($gt$ stands for ground truth). We will try to estimate these values based on our observations and let us see how close we get to \"actual\" parameters. Note that in reality we obviously don't have these parameters as that is exactly what we want to estimate in the first place. So let us consider the following setup, we have:\n",
    "\n",
    "- Number of observations, $num\\_obs = 50$\n",
    "- Our 50 set of observations would be\n",
    "    - $x_{obs} = np.linspace(-25,25, num\\_obs)$\n",
    "    - $y_{obs} = y(x_{obs};a_{gt},m_{gt},s_{gt})$  from $(1)$\n",
    "\n",
    "Reference:\n",
    "\n",
    "â†’[linspace](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html)\n",
    "\n",
    "- Say we are given initial estimate as:\n",
    "\n",
    "    $$a_0=10; \\quad m_0=13; \\quad s_0=19.12$$\n",
    "\n",
    "### Residual and error to be minimized\n",
    "\n",
    "Okay, now we have set of observations and an initial estimate of parameters. We would now want to minimize an error that would give us optimum parameters.\n",
    "\n",
    "The $residual$ would be given by\n",
    "\n",
    "$$ r(a,m,s) = \\left[ a \\exp \\left(\\frac{-(x_{obs}-m)^{2}}{2 s^{2}}\\right) - y_{obs}\\ \\right]$$\n",
    "\n",
    "where we'd want to minimize $\\|r\\|^2$. Note that $r$ is a non-linear function in $(a,m,s)$.\n",
    "\n",
    "Also, note that since $y$ (and $x$) are observations in the above equation, after simplification, we get $\\mathbf{J}_r = \\mathbf{J}_y$ [above](https://www.notion.so/c9e6f71b67a44bb8b366df2fccfc12d0) (since $y_{obs}$ is a constant).\n",
    "\n",
    "Let us apply Gradient Descent method for minimization here. From [Table I](https://www.notion.so/From-linear-algebra-to-non-linear-weighted-least-squares-optimization-13cf17d318be4d45bb8577c4d3ea4a02),  \n",
    "\n",
    "$$\\Delta \\mathbf{k} = - \\alpha \\mathbf{J_F} = -\\alpha \\mathbf{J}_r^{\\top} {r}(\\mathbf{k})$$\n",
    "\n",
    "Note that $\\mathbf{J_F}$ is the Jacobian of \"non-linear least squares\" function $\\mathbf{F}$ while $\\mathbf{J}_r$ is the Jacobian of the residual. \n",
    "\n",
    "where $\\mathbf{k}$ is $[a,m,s]^T$. \n",
    "\n",
    "- Some hyperparameters:\n",
    "    - Learning rate, $lr = 0.01$\n",
    "    - Maximum number of iterations, $num\\_iter=200$\n",
    "    - Tolerance, $tol = 1e-15$\n",
    "\n",
    "## Solution for 1 iteration\n",
    "\n",
    "To see how each step looks like, let us solve for 1 iteration and for simpler calculations, assume we have 3 observations, \n",
    "\n",
    "$$x_{obs}= \\left[ -25, 0, 25 \\right]^T, y_{obs} = \\left[  4.5783, 10, 4.5783 \\right]^T. $$\n",
    "\n",
    "With our initial estimate as $\\mathbf{k_0} = [a_0=10, \\quad m_0=13, \\quad s_0=19.12]^T$, the residual would be \n",
    "\n",
    "$$ r(a_0,m_0,s_0) = \\left[ a_0 \\exp \\left(\\frac{-(x_{obs}-m_0)^{2}}{2 s_0^{2}}\\right) - y_{obs}\\ \\right]$$\n",
    "\n",
    "Therefore, $r=[-3.19068466, -2.0637411 , 3.63398058]^T$.\n",
    "\n",
    "### Gradient Computation\n",
    "\n",
    "Gradient, $\\mathbf{J_F}$=\n",
    "\n",
    "$$\\mathbf{J_r}^{\\top} \\mathbf{r}(\\mathbf{k})$$\n",
    "\n",
    "We have calculated residual already [above](https://www.notion.so/c9e6f71b67a44bb8b366df2fccfc12d0), let us calculate the Jacobian $\\mathbf{J_r}$.\n",
    "\n",
    "$$\\mathbf{J}_r\n",
    "= \\left[ \\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right); \\frac{a (x-m)}{s^2} \\exp\\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right);  \\frac{a (x-m)^2}{s^3}\\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right)\\right]$$\n",
    "\n",
    "$$\\implies \\mathbf{J_r} = \\left[ \\begin{array}{rrr}0.1387649 & 0.79362589, & 0.82123142 \\\\-0.14424057 & -0.28221715  & 0.26956967 \\\\0.28667059 & 0.19188405, & 0.16918599\\end{array}\\right]$$\n",
    "\n",
    "So ,\n",
    "\n",
    "$$\\mathbf{J_F} = \\mathbf{J_r}^{\\top} \\mathbf{r}(\\mathbf{k})$$\n",
    "\n",
    "$$\\mathbf{r}(\\mathbf{k}) =  \\left[ \\begin{array}{r}-3.19068466 \\\\ -2.0637411 \\\\ 3.63398058 \\end{array} \\right]$$\n",
    "\n",
    "$$ \\begin{aligned} \\implies \\mathbf{J_F} = \\left[ \\begin{array}{r} 0.89667553 \\\\ -1.25248392 \\\\-2.56179392\\end{array} \\right] \\end{aligned}$$\n",
    "\n",
    "### Update step\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{k} = - \\alpha \\mathbf{J_F} \\\\\n",
    "\\mathbf{k}^{t+1} = \\mathbf{k}^t + \\Delta \\mathbf{k}\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ our learning rate is 0.01.\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{k} = - \\alpha\\times\\left[ \\begin{array}{r} \n",
    "0.89667553 \\\\ -1.25248392 \\\\-2.56179392\n",
    "\\end{array} \\right] = \\left[ \\begin{array}{r}\n",
    "-0.00896676 \\\\ 0.01252484 \\\\0.02561794\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{k}^{1} = \\mathbf{k}^{0} + \\Delta \\mathbf{k} \\\\ \\left[\\begin{array}{r} 10 \\\\ 13 \\\\ 19.12 \\end{array}\\right] + \\left[\\begin{array}{c} 9.99103324 \\\\ 13.01252484 \\\\ 19.14561794 \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "With just one iteration with very few observations, we can see that we have gotten *slightly* more closer to our GT parameter  $a_{gt}=10; m_{gt} =0; s_{gt} =20$. Our initial estimate was $[a_0=10, \\quad m_0=13, \\quad s_0=19.12]$. However, the above might not be noticeable enough: Hence you need to code it for more iterations and convince yourself as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.func import make_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f4819d",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ed5487d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_gaussian' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5df1d87e410a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0myobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_gaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0myest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_gaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19.12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_gaussian' is not defined"
     ]
    }
   ],
   "source": [
    "xobs = np.linspace(-25, 25, 50)\n",
    "yobs = []\n",
    "yest = []\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "yest = make_gaussian(xobs, 10, 13, 19.12)\n",
    "\n",
    "plt.scatter(xobs, yobs)\n",
    "plt.scatter(xobs, yest)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(['Ground Truth', 'Initial Estimate'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81d8de",
   "metadata": {},
   "source": [
    "## Defining the partial derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aaad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_der(x, a, mean, std):\n",
    "    y = make_gaussian(x, a, mean, std)/a\n",
    "    return y\n",
    "\n",
    "def m_der(x, a, mean, std):\n",
    "    y = make_gaussian(x, a, mean, std) * ((x-mean)/(std**2))\n",
    "    return y\n",
    "\n",
    "def s_der(x, a, mean, std):\n",
    "    y = make_gaussian(x, a, mean, std) * (-1/std) + make_gaussian(x, a, mean, std) * ((x-mean)**2/(std**3))\n",
    "    return y\n",
    "\n",
    "def jacobian(x, a, m, s):\n",
    "    return np.c_[a_der(x, a, m, s), m_der(x, a, m, s), s_der(x, a, m, s)]\n",
    "    \n",
    "def residual(x, y, a, m, s):\n",
    "    return make_gaussian(x, a, m, s) - y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e795f5",
   "metadata": {},
   "source": [
    "## Function for Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(xobs, yobs, a, m, s, max_iterations, learning_rate, tolerance):\n",
    "    \n",
    "    weights = np.array([a, m, s], dtype=np.double)\n",
    "    errors = []\n",
    "    itr = []\n",
    "    total_iterations = max_iterations\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        J = jacobian(xobs, weights[0], weights[1], weights[2])\n",
    "        R = residual(xobs, yobs, weights[0], weights[1], weights[2])\n",
    "        weights = weights - learning_rate * np.dot(J.T, R)\n",
    "        \n",
    "        error = np.sqrt(np.sum(np.square(yobs-make_gaussian(xobs, weights[0], weights[1], weights[2]))) / 50)\n",
    "        \n",
    "        if error<tolerance:\n",
    "            total_iterations = _\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            errors.append(error)\n",
    "            itr.append(_)\n",
    "        \n",
    "    return weights[0], weights[1], weights[2], total_iterations, errors, itr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d04cc94",
   "metadata": {},
   "source": [
    "## Sample run for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08130778",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_iterations = 2000\n",
    "learning_rate = 25\n",
    "tolerance = 1e-5\n",
    "a_est = 10\n",
    "m_est = 13\n",
    "s_est = 19.12\n",
    "\n",
    "a, m, s, tot_itr, cost, itr = gradient_descent(xobs, yobs, a_est, m_est, s_est, max_iterations, learning_rate, tolerance)\n",
    "print(\"Total iterations till convergence: \" + str(tot_itr))\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "yest = make_gaussian(xobs, a, m, s)\n",
    "\n",
    "plt.scatter(xobs, yobs)\n",
    "plt.scatter(xobs, yest)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(['Ground Truth', 'Estimate'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac86aa2",
   "metadata": {},
   "source": [
    "## Experimenting with number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41246f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterations_list = [200, 400, 600, 800, 1000, 2753]\n",
    "\n",
    "for _ in iterations_list:\n",
    "    \n",
    "    # Hyper - parameters\n",
    "    max_iterations = _\n",
    "    learning_rate = 5\n",
    "    tolerance = 0.0001\n",
    "    a_est = 10\n",
    "    m_est = 13\n",
    "    s_est = 19.12\n",
    "\n",
    "    a, m, s, tot_itr, cost, itr = gradient_descent(xobs, yobs, a_est, m_est, s_est, max_iterations, learning_rate, tolerance)\n",
    "    print(\"Learning rate = 15\")\n",
    "    print(\"Tolerance = 0.0001\")\n",
    "    print(\"Total iterations run for = \" + str(_))\n",
    "        \n",
    "    try:\n",
    "        print(\"Error after \"+str(_)+\" iterations = \"+str(cost[-1]))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"Ground Truth vs Predicted values plot after \"+str(tot_itr)+\" iterations:\")\n",
    "    plt.clf()\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, a, m, s)\n",
    "    plt.scatter(xobs, yobs)\n",
    "    plt.scatter(xobs, yest)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(['Ground Truth', 'Estimate'])\n",
    "    plt.show()\n",
    "    \n",
    "    if len(itr)>0:\n",
    "        print(\"Plot for error vs iterations:\")\n",
    "        plt.clf()\n",
    "        plt.plot(itr, cost)\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No graph for error vs iterations as no iterations were run.\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ff17a",
   "metadata": {},
   "source": [
    "In the above plots, we can see that the shape of the graph remains the same as the tolerance value and learning rate are constant but as the total number of iterations are increased the graph gets extended on the X-axis and the error keeps reducing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b8b5b",
   "metadata": {},
   "source": [
    "## Experimenting with learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01baa0be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_list = [2, 4, 6, 8, 10, 15]\n",
    "\n",
    "for _ in learning_list:\n",
    "    \n",
    "    # Hyper - parameters\n",
    "    max_iterations = 2000\n",
    "    learning_rate = _\n",
    "    tolerance = 0.0001\n",
    "    a_est = 10\n",
    "    m_est = 13\n",
    "    s_est = 19.12\n",
    "\n",
    "    a, m, s, tot_itr, cost, itr = gradient_descent(xobs, yobs, a_est, m_est, s_est, max_iterations, learning_rate, tolerance)\n",
    "    print(\"Learning rate = \"+str(_))\n",
    "    print(\"Tolerance = 0.0001\")\n",
    "    print(\"Total iterations run for = \" + str(tot_itr))\n",
    "    if tot_itr==2000: print(\"The algorithm didn't converge even after 2000 iterations.\")\n",
    "        \n",
    "    try:\n",
    "        print(\"Error after \"+str(tot_itr)+\" iterations = \"+str(cost[-1]))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"Ground Truth vs Predicted values plot after \"+str(tot_itr)+\" iterations:\")\n",
    "    plt.clf()\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, a, m, s)\n",
    "    plt.scatter(xobs, yobs)\n",
    "    plt.scatter(xobs, yest)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(['Ground Truth', 'Estimate'])\n",
    "    plt.show()\n",
    "    \n",
    "    if len(itr)>0:\n",
    "        print(\"Plot for error vs iterations:\")\n",
    "        plt.clf()\n",
    "        plt.plot(itr, cost)\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No graph for error vs iterations as no iterations were run.\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae22820f",
   "metadata": {},
   "source": [
    "In the above graphs we can see that the shape of the graph changes and becomes more steep for higher learning rate. That is, the error falls faster per iteration, when learning rate is higher. Hence, higher learning rate takes lower number of iterations to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62b895",
   "metadata": {},
   "source": [
    "## Experimenting with tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbae4ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tolerance_list = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "\n",
    "for _ in tolerance_list:\n",
    "    \n",
    "    # Hyper - parameters\n",
    "    max_iterations = 2000\n",
    "    learning_rate = 15\n",
    "    tolerance = _\n",
    "    a_est = 10\n",
    "    m_est = 13\n",
    "    s_est = 19.12\n",
    "\n",
    "    a, m, s, tot_itr, cost, itr = gradient_descent(xobs, yobs, a_est, m_est, s_est, max_iterations, learning_rate, tolerance)\n",
    "    print(\"Tolerance = \"+str(_))\n",
    "    print(\"Learning rate = 15\")\n",
    "    print(\"Total iterations run for = \" + str(tot_itr))\n",
    "    if tot_itr==2000: print(\"The algorithm is slow as it didn't converge even after 2000 iterations\")\n",
    "\n",
    "    try:\n",
    "        print(\"Error after \"+str(tot_itr)+\" iterations = \"+str(cost[-1]))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"Ground Truth vs Predicted values plot after \"+str(tot_itr)+\" iterations:\")\n",
    "    plt.clf()\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, a, m, s)\n",
    "    plt.scatter(xobs, yobs)\n",
    "    plt.scatter(xobs, yest)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(['Ground Truth', 'Estimate'])\n",
    "    plt.show()\n",
    "    \n",
    "    if len(itr)>0:\n",
    "        print(\"Plot for error vs iterations:\")\n",
    "        plt.clf()\n",
    "        plt.plot(itr, cost)\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No graph for error vs iterations as no iterations were run.\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f30a6",
   "metadata": {},
   "source": [
    "In the above graphs we can see that the shape of the graph remains the same as the learning rate is same for all the plots. The only difference is that for lower tolerance value, the algorithm runs for more iterations and hence the graph gets extended on the X-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d29a30",
   "metadata": {},
   "source": [
    "## 1.2: Another Non-Linear function\n",
    "Now that you've got the hang of computing the jacobian matrix for a non-linear function via the aid of an example, try to compute the jacobian of a secondary gaussian function by carrying out steps similar to what has been shown above. The function is plotted below:\n",
    "<img src='./helpers/non_linear.png' alt=drawing width=500 height=600>\n",
    "Using the computed jacobian, optimise for the four parameters using gradient descent, where the parameters to be estimated are: \n",
    "\n",
    "$p_1$ = 2,  $p_2$ = 8,  $p_3$ = 4,  $p_4$ = 8. \n",
    "\n",
    "Do this for $x_{obs} = np.linspace(-20,30, num\\_obs)$,\n",
    "where $num\\_obs$ is 50.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc9dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.func import make_non_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2153f785",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f93c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xobs = np.linspace(-20, 30, 50)\n",
    "yobs = []\n",
    "yest = []\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "for i in xobs:\n",
    "    yobs.append(make_non_linear(i, 2, 8, 4, 8))\n",
    "    \n",
    "for i in xobs:\n",
    "    yest.append(make_non_linear(i, 4, 6, 2, 6))\n",
    "\n",
    "plt.scatter(xobs, yobs)\n",
    "plt.scatter(xobs, yest)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(['Ground Truth', 'Initial Estimate'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c852ef",
   "metadata": {},
   "source": [
    "## Defining the partial derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150dc01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p1_der(x, p1, p2, p3, p4):\n",
    "    return  np.exp(-x / p2) \n",
    "\n",
    "def p2_der(x, p1, p2, p3, p4):\n",
    "    return  p1 * np.exp(-x / p2) * (x/(p2**2)) \n",
    "\n",
    "def p3_der(x, p1, p2, p3, p4):\n",
    "    return  np.sin(x / p4) \n",
    "\n",
    "def p4_der(x, p1, p2, p3, p4):\n",
    "    return  p3 * np.cos(x / p4) * (-x/(p4**2)) \n",
    "\n",
    "def jacobian1(x, p1, p2, p3, p4):\n",
    "    return np.c_[p1_der(x, p1, p2, p3, p4), p2_der(x, p1, p2, p3, p4), p3_der(x, p1, p2, p3, p4), p4_der(x, p1, p2, p3, p4)]\n",
    "    \n",
    "def residual1(x, y, p1, p2, p3, p4):\n",
    "    y_guess = []\n",
    "    for i in xobs:\n",
    "        y_guess.append(make_non_linear(i, p1, p2, p3, p4))\n",
    "    return np.subtract(np.array(y_guess), np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03fbf02",
   "metadata": {},
   "source": [
    "## Function for Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabfca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent1(xobs, yobs, p1, p2, p3, p4, max_iterations, learning_rate, tolerance):\n",
    "    \n",
    "    weights = np.array([p1, p2, p3, p4], dtype=np.double)\n",
    "    errors = []\n",
    "    itr = []\n",
    "    total_iterations = max_iterations\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "\n",
    "        J = jacobian1(xobs, weights[0], weights[1], weights[2], weights[3])\n",
    "        R = residual1(xobs, yobs, weights[0], weights[1], weights[2], weights[3])\n",
    "        weights = weights - learning_rate * np.dot(J.T, R)\n",
    "        \n",
    "        y_guess = []\n",
    "        for i in xobs:\n",
    "            y_guess.append(make_non_linear(i, weights[0], weights[1], weights[2], weights[3]))\n",
    "        \n",
    "        error = np.sqrt(np.sum(np.square(yobs-np.array(y_guess))) / 50)\n",
    "        \n",
    "        if error<tolerance:\n",
    "            total_iterations = _\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            errors.append(error)\n",
    "            itr.append(_)\n",
    "        \n",
    "    return weights[0], weights[1], weights[2], weights[3], total_iterations, errors, itr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca7bc8",
   "metadata": {},
   "source": [
    "## Running the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf503519",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "max_iterations = 5000\n",
    "learning_rate = 1e-4\n",
    "tolerance = 1e-2\n",
    "\n",
    "p1 = 4\n",
    "p2 = 6\n",
    "p3 = 2\n",
    "p4 = 6\n",
    "\n",
    "p1, p2, p3, p4, tot_itr, cost, itr = gradient_descent1(xobs, yobs, p1, p2, p3, p4, max_iterations, learning_rate, tolerance)\n",
    "print(\"Total iterations till convergence: \" + str(tot_itr))\n",
    "\n",
    "plt.clf()\n",
    "yobs = []\n",
    "yest = []\n",
    "plt.clf()\n",
    "\n",
    "for i in xobs:\n",
    "    yobs.append(make_non_linear(i, 2, 8, 4, 8))\n",
    "    \n",
    "for i in xobs:\n",
    "    yest.append(make_non_linear(i, p1, p2, p3, p4))\n",
    "\n",
    "plt.scatter(xobs, yobs)\n",
    "plt.scatter(xobs, yest)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(['Ground Truth', 'Estimate'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817fc608",
   "metadata": {},
   "source": [
    "## 1.3: Different Optimizers\n",
    "\n",
    "Replace gradient descent with Gauss-Newton and Levenberg Marquardt algorithms and repeat question 1.1. \n",
    "\n",
    "To quickly recap, Gauss-Newton and Levenberg Marquardt are alternate update rules to the standard gradient descent. Gauss Newton updates work as:\n",
    "\n",
    "$$\\delta x = -(J^TJ)^{-1}J^Tf(x)$$\n",
    "\n",
    "Levenberg Marquardt lies somewhere between Gauss Newton and Gradient Descent algorithms by blending the two formulations. As a result, when at a steep cliff, LM takes small steps to avoid overshooting, and when at a gentle slope, LM takes bigger steps:\n",
    "\n",
    "\n",
    "$$\\delta x = -(J^TJ + \\lambda I)^{-1}J^Tf(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3fbf4f",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "1. How does the choice of initial estimate and learning rate affect convergence? Observations and analysis from repeated runs with modified hyperparameters will suffice.\n",
    "2. Do you notice any difference between the three optimizers? Why do you think that is? (If you are unable to see a clear trend, what would you expect in general based on what you know about them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bfdceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_newton(xobs, yobs, a, m, s, tolerance, max_iterations):\n",
    "    \n",
    "    weights = np.array([a, m, s], dtype=np.double)\n",
    "    total_iterations = max_iterations\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        J = jacobian(xobs, weights[0], weights[1], weights[2])\n",
    "        R = residual(xobs, yobs, weights[0], weights[1], weights[2])\n",
    "        weights = weights - np.linalg.pinv(J.T @ J) @ J.T @ R\n",
    "        error = np.sqrt(np.sum(np.square(yobs-make_gaussian(xobs, weights[0], weights[1], weights[2]))) / 50)\n",
    "        \n",
    "        if error<tolerance:\n",
    "            total_iterations = _\n",
    "            break\n",
    "        \n",
    "    return weights[0], weights[1], weights[2], total_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0e719",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimates_list = [[15, 5, 25], [5, -5, 15], [21, 11, 31], [-5, -15, 5]]\n",
    "\n",
    "print(\"\\nRunning with tolerance = 1e-6\\n\\n\")\n",
    "\n",
    "for _ in estimates_list:\n",
    "\n",
    "    xobs = np.linspace(-25, 25, 50)\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, _[0], _[1], _[2])\n",
    "\n",
    "    tolerance = 1e-6\n",
    "    max_iterations = 1000\n",
    "    a, m, s , tot_iter = gauss_newton(xobs, yobs, _[0], _[1], _[2], tolerance, max_iterations)\n",
    "\n",
    "    print(\"Initial estimates [a, m, s] were \"+str(_))\n",
    "    print(\"Total iterations taken are = \"+str(tot_iter))\n",
    "\n",
    "    if tot_iter==max_iterations:\n",
    "        print(\"The algorithm didn't converge in \"+str(max_iterations)+\" iterations.\")\n",
    "    \n",
    "    plt.clf()\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, a, m, s)\n",
    "    plt.scatter(xobs, yobs)\n",
    "    plt.scatter(xobs, yest)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(['Ground Truth', 'Estimate'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968e790",
   "metadata": {},
   "source": [
    "**Observations and analysis with changed initial estimates and tolerance = 1e-5**  \n",
    "It can be seen that as we take our initial estimates away from the correct value for the parameters, the algorithm takes more iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326be780",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimates_list = [[15, 5, 25], [5, -5, 15], [21, 11, 31], [-5, -15, 5]]\n",
    "\n",
    "print(\"\\nRunning with tolerance = 1e-15\\n\\n\")\n",
    "\n",
    "for _ in estimates_list:\n",
    "\n",
    "    xobs = np.linspace(-25, 25, 50)\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, _[0], _[1], _[2])\n",
    "\n",
    "    tolerance = 1e-15\n",
    "    max_iterations = 10000\n",
    "    a, m, s , tot_iter = gauss_newton(xobs, yobs, _[0], _[1], _[2], tolerance, max_iterations)\n",
    "\n",
    "    print(\"Initial estimates [a, m, s] were \"+str(_))\n",
    "    print(\"Total iterations taken are = \"+str(tot_iter))\n",
    "    \n",
    "    if tot_iter==max_iterations:\n",
    "        print(\"The algorithm didn't converge in \"+str(max_iterations)+\" iterations.\")\n",
    "\n",
    "    \n",
    "    plt.clf()\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, a, m, s)\n",
    "    plt.scatter(xobs, yobs)\n",
    "    plt.scatter(xobs, yest)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(['Ground Truth', 'Estimate'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c53aa4",
   "metadata": {},
   "source": [
    "**Observations and analysis with changed initial estimates and tolerance = 1e-15**  \n",
    "It can be seen that as we take our initial estimates away from the correct value for the parameters, the algorithm takes more iterations to converge. Another thing to notice is that the algorithm quickly reached error below 1e-5 and **reached 1e-15 error in only one more iteration**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenberg_marquardt(xobs, yobs, a, m, s, lmda, tolerance, max_iterations):\n",
    "    \n",
    "    total_iterations = max_iterations\n",
    "    weights = np.array([a, m, s], dtype=np.double)\n",
    "    errors = [np.linalg.norm(residual(xobs, yobs, weights[0], weights[1], weights[2])) ** 2, ]\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        J = jacobian(xobs, weights[0], weights[1], weights[2])\n",
    "        R = residual(xobs, yobs, weights[0], weights[1], weights[2])\n",
    "        new_error = np.linalg.norm(R) ** 2\n",
    "        weights = weights - np.linalg.pinv((J.T @ J) + (lmda * np.eye(J.shape[1]))) @ J.T @ R\n",
    "        \n",
    "        if len(errors) > 0:\n",
    "            if new_error > errors[-1]:\n",
    "                lmda = lmda * 2\n",
    "            else:\n",
    "                lmda = lmda / 3\n",
    "                \n",
    "        errors.append(new_error)\n",
    "        \n",
    "        if new_error<tolerance:\n",
    "            total_iterations = _\n",
    "            break\n",
    "        \n",
    "    return weights[0], weights[1], weights[2], total_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6aba3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimates_list = [[15, 5, 25], [5, -5, 15], [21, 11, 31], [-5, -15, 5]]\n",
    "\n",
    "print(\"\\nRunning with tolerance = 1e-6\\n\\n\")\n",
    "\n",
    "for _ in estimates_list:\n",
    "  \n",
    "    xobs = np.linspace(-25, 25, 50)\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, _[0], _[1], _[2])\n",
    "\n",
    "    lmda = 1\n",
    "    tolerance = 1e-6\n",
    "    max_iterations = 100\n",
    "\n",
    "    a, m, s, tot_iter = levenberg_marquardt(xobs, yobs, _[0], _[1], _[2], lmda, tolerance, max_iterations)\n",
    "    plt.clf()\n",
    "    \n",
    "    print(\"Initial estimates [a, m, s] were \"+str(_))\n",
    "    print(\"Total iterations taken are = \"+str(tot_iter))\n",
    "    \n",
    "    if tot_iter==max_iterations:\n",
    "        print(\"The algorithm didn't converge in \"+str(max_iterations)+\" iterations.\")\n",
    "\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, a, m, s)\n",
    "    plt.scatter(xobs, yobs)\n",
    "    plt.scatter(xobs, yest)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(['Ground Truth', 'Initial Estimate'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99bff7",
   "metadata": {},
   "source": [
    "**Observations and analysis with changed initial estimates and tolerance = 1e-6**  \n",
    "It can be seen that as we take our initial estimates away from the correct value for the parameters, the algorithm takes more iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81a42c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimates_list = [[15, 5, 25], [5, -5, 15], [21, 11, 31], [-5, -15, 5]]\n",
    "\n",
    "print(\"\\nRunning with tolerance = 1e-15\\n\\n\")\n",
    "\n",
    "for _ in estimates_list:\n",
    "  \n",
    "    xobs = np.linspace(-25, 25, 50)\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, _[0], _[1], _[2])\n",
    "\n",
    "    lmda = 1\n",
    "    tolerance = 1e-15\n",
    "    max_iterations = 100\n",
    "\n",
    "    a, m, s, tot_iter = levenberg_marquardt(xobs, yobs, _[0], _[1], _[2], lmda, tolerance, max_iterations)\n",
    "    plt.clf()\n",
    "    \n",
    "    print(\"Initial estimates [a, m, s] were \"+str(_))\n",
    "    print(\"Total iterations taken are = \"+str(tot_iter))\n",
    "    \n",
    "    if tot_iter==max_iterations:\n",
    "        print(\"The algorithm didn't converge in \"+str(max_iterations)+\" iterations.\")\n",
    "\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, a, m, s)\n",
    "    plt.scatter(xobs, yobs)\n",
    "    plt.scatter(xobs, yest)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(['Ground Truth', 'Initial Estimate'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae73d3a",
   "metadata": {},
   "source": [
    "**Observations and analysis with changed initial estimates and tolerance = 1e-15**  \n",
    "It can be seen that as we take our initial estimates away from the correct value for the parameters, the algorithm takes more iterations to converge. Also the algorithm **takes about 2 to 3 iterations in each case for the error to go from 1e-6 to 1e-15**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b695961",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimates_list = [[15, 5, 25], [5, -5, 15], [21, 11, 31], [-5, -15, 5]]\n",
    "\n",
    "print(\"\\nRunning with initial labmda = 10\\n\\n\")\n",
    "\n",
    "for _ in estimates_list:\n",
    "  \n",
    "    xobs = np.linspace(-25, 25, 50)\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, _[0], _[1], _[2])\n",
    "\n",
    "    lmda = 10\n",
    "    tolerance = 1e-5\n",
    "    max_iterations = 100\n",
    "\n",
    "    a, m, s, tot_iter = levenberg_marquardt(xobs, yobs, _[0], _[1], _[2], lmda, tolerance, max_iterations)\n",
    "    plt.clf()\n",
    "    \n",
    "    print(\"Initial estimates [a, m, s] were \"+str(_))\n",
    "    print(\"Total iterations taken are = \"+str(tot_iter))\n",
    "    \n",
    "    if tot_iter==max_iterations:\n",
    "        print(\"The algorithm didn't converge in \"+str(max_iterations)+\" iterations.\")\n",
    "\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, a, m, s)\n",
    "    plt.scatter(xobs, yobs)\n",
    "    plt.scatter(xobs, yest)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(['Ground Truth', 'Initial Estimate'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d391ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimates_list = [[15, 5, 25], [5, -5, 15], [21, 11, 31], [-5, -15, 5]]\n",
    "\n",
    "print(\"\\nRunning with lambda = 100\\n\\n\")\n",
    "\n",
    "for _ in estimates_list:\n",
    "  \n",
    "    xobs = np.linspace(-25, 25, 50)\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, _[0], _[1], _[2])\n",
    "\n",
    "    lmda = 100\n",
    "    tolerance = 1e-6\n",
    "    max_iterations = 100\n",
    "\n",
    "    a, m, s, tot_iter = levenberg_marquardt(xobs, yobs, _[0], _[1], _[2], lmda, tolerance, max_iterations)\n",
    "    plt.clf()\n",
    "    \n",
    "    print(\"Initial estimates [a, m, s] were \"+str(_))\n",
    "    print(\"Total iterations taken are = \"+str(tot_iter))\n",
    "    \n",
    "    if tot_iter==max_iterations:\n",
    "        print(\"The algorithm didn't converge in \"+str(max_iterations)+\" iterations.\")\n",
    "\n",
    "    yobs = make_gaussian(xobs, 10, 0, 20)\n",
    "    yest = make_gaussian(xobs, a, m, s)\n",
    "    plt.scatter(xobs, yobs)\n",
    "    plt.scatter(xobs, yest)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(['Ground Truth', 'Initial Estimate'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489e298",
   "metadata": {},
   "source": [
    "**Observations and analysis with changed initial estimates and lambda**  \n",
    "From the above runs, we can tell that lamda = 10 was a better estimate to initialise the variable as lambda = 100 took on an average 2 to 3 more iterations in every case to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d5585",
   "metadata": {},
   "source": [
    "**Do you notice any difference between the three optimizers? Why do you think that is? (If you are unable to see a clear trend, what would you expect in general based on what you know about them.)**  \n",
    "  \n",
    "The biggest noticeable difference we can see is that gradient descent takes much more iterations compared to Gauss Newton and Levenberg Marquardt. This is because the jacobian is very small in gradient descent, and learning rate can't be kept very high as the convergence won't be neat otherwise. This explains the high number of iterations. Between Gauss Newton and Levenberg Marquardt, we know Levenberg Marquardt is a kind of formulation of Gauss Newton and hence the results are almost similar. In our cases, it is seen that Gauss Newton takes fewer iterations, however, this might be due to inefficient initialisation of lambda in Levenberg Marquardt and does not indicate that Gauss Newton is better. Both the algorithms are computationally more expensive but give required result in fewer iterations compared to gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17447c53",
   "metadata": {},
   "source": [
    "# 2. Iterative Closest Point\n",
    "\n",
    "In this subsection, we will code the Iterative Closest Point algorithm to find the alignment between two point clouds without known correspondences. The point cloud that you will be using is the same as the one that you used in Assignment 1.\n",
    "\n",
    "## 2.1: Procrustes alignment\n",
    "\n",
    "1. Write a function that takes two point clouds as input wherein the corresponding points between the two point clouds are located at the same index and returns the transformation matrix between them.\n",
    "2. Use the bunny point cloud and perform the procrustes alignment between the two bunnies. Compute the absolute alignment error after aligning the two bunnies.\n",
    "3. Make sure your code is modular as we will use this function in the next sub-part.\n",
    "4. Prove mathematically why the Procrustes alignment gives the best aligning transform between point clouds with known correspondences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece3d19",
   "metadata": {},
   "source": [
    "**Generating initial point clouds and visualizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdccd4dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "import open3d as o3d\n",
    "\n",
    "pcd = o3d.io.read_point_cloud(\"./bunny.pcd\")\n",
    "X = np.asarray(pcd.points)  \n",
    "X = X.T\n",
    "\n",
    "# Tuning the PCD a bit away form the origin \n",
    "X[0] = X[0] + 0.5\n",
    "X[1] = X[1] + 0.5\n",
    "X[2] = X[2] + 0.5\n",
    "\n",
    "# Downsampling 25 times\n",
    "X = X[:,::25]\n",
    "\n",
    "# Getting the second point cloud and shuffling all the points\n",
    "P = np.copy(X)\n",
    "\n",
    "# Translating the second point cloud\n",
    "P[0,:] = P[0,:] + .25\n",
    "P[1,:] = P[1,:] + .25\n",
    "P[2,:] = P[2,:] + .25\n",
    "\n",
    "# Rotating the second point cloud\n",
    "theta1 = ( 90.0 / 360) * 2 * np.pi\n",
    "rot1 = np.array([[math.cos(theta1), -math.sin(theta1),0],\n",
    "                 [math.sin(theta1),  math.cos(theta1),0],\n",
    "               [0,0,1]])\n",
    "P1 = np.dot(rot1, P)\n",
    "\n",
    "# Visualzing the bunnies\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(X.T)\n",
    "pcd.paint_uniform_color([1, 0, 0])\n",
    "pcd1.points = o3d.utility.Vector3dVector(P1.T)\n",
    "pcd1.paint_uniform_color([0, 0, 1])\n",
    "o3d.visualization.draw_geometries([pcd,pcd1],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d70d4",
   "metadata": {},
   "source": [
    "**Function to perform Procrustes alignment and get transformation matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2090326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Procrustes(X, P):\n",
    "    \n",
    "    total_iterations = 5    # Set number of iterations here\n",
    "    n = X.shape[1]\n",
    "    dim = X.shape[0]\n",
    "    T = np.identity(4)\n",
    "    \n",
    "    error = np.sqrt(np.sum(np.square(X-P)) / X.shape[1])\n",
    "    print('Initial error: '+str(error))\n",
    "    \n",
    "    for iteration in range(total_iterations):\n",
    "        \n",
    "        W = np.zeros((3,3))\n",
    "        point_cloud1_mean = np.mean(X, axis = 1, keepdims = True)\n",
    "        point_cloud2_mean = np.mean(P, axis = 1, keepdims = True)\n",
    "\n",
    "        for idx in range(n):\n",
    "            point1 = X[:, idx]\n",
    "            point1 = point1.reshape(3,1)\n",
    "            arr1 = point1 - point_cloud1_mean\n",
    "            point2 = P[:, idx]\n",
    "            point2 = point2.reshape(3,1)\n",
    "            arr2 = point2 - point_cloud2_mean\n",
    "            W = W+np.dot(arr1,arr2.T)\n",
    "\n",
    "        W = W/n\n",
    "        U, S, Vt = np.linalg.svd(W)\n",
    "        R = np.dot(U,Vt)\n",
    "\n",
    "        if np.linalg.det(R) < 0:\n",
    "            Vt[dim - 1, :]*=-1\n",
    "            R = U @ Vt\n",
    "        \n",
    "        t = point_cloud1_mean - np.dot( R , point_cloud2_mean)\n",
    "        \n",
    "        T_itr = R\n",
    "        T_itr = np.append(T_itr, t, axis=1)\n",
    "        T_itr = np.vstack([T_itr, [0, 0, 0, 1]])\n",
    "        T = np.dot(T_itr, T)\n",
    "        P = np.dot(R, P) + t\n",
    "        \n",
    "        error = np.sqrt(np.sum(np.square(X-P)) / X.shape[1])\n",
    "        print('Error after iteration '+str(iteration+1)+ ': '+str(error))\n",
    "\n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e749f7",
   "metadata": {},
   "source": [
    "**Performing Procrustes alignment on 2 bunny point clouds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2726f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ggez = P1\n",
    "T = Procrustes(X,P1)\n",
    "\n",
    "ones = np.ones(P1.shape[1])\n",
    "P1 = np.vstack([P1, ones])\n",
    "P1 = np.dot(T, P1)\n",
    "P1 = P1[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab2dd0",
   "metadata": {},
   "source": [
    "**Visualizing the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(X.T)\n",
    "pcd.paint_uniform_color([1, 0, 0])\n",
    "pcd1.points = o3d.utility.Vector3dVector(P1.T)\n",
    "pcd1.paint_uniform_color([0, 0, 1])\n",
    "o3d.visualization.draw_geometries([pcd,pcd1],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba2678",
   "metadata": {},
   "source": [
    "**Prove mathematically why the Procrustes alignment gives the best aligning transform between point clouds with known correspondences.**  \n",
    "For point clouds P and Q, the error function we minimize during procrustes alignment, for calculating rotation R and transformation T is $\\sum_{}$ || (RP<sub>i</sub> + T) - Q<sub>i</sub> ||<sup>2</sup>. The mininimum value for this error function will be 0 when the two point clouds will be perfectly aligned. Since after perfect alignment, these points will co-incide and distances between corresponding points will be 0. On further simplification, by substituting T in terms of R, this error function transforms to `arg min R for ||R(P-P')-(Q-Q')||`. Now minimizing this is equivalent to minimizing the \"Orthogonal Procrustes\" problem, that has a standard solution through SVD. On solving through the SVD method, we get the R for which the error function will be minimum, and from R we get T. Now as the error function is minimum for these R and T, it means it is 0 (since minimum value of the error is 0). We can be sure hence be sure due to our error function, that this R and T give the best alignment between the 2 point clouds as all the corresponding points must be coinciding.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98699f13",
   "metadata": {},
   "source": [
    "## 2.2: ICP alignment\n",
    "\n",
    "1. Write a function that takes two point clouds as input without known correspondences and perform the iterative closest point algorithm.\n",
    "2. Perform the ICP alignment between the two bunnies and plot their individual coordinate frames as done in class.\n",
    "3. Does ICP always give the correct alignment? Why or Why not?\n",
    "4. What are other variants of ICP and why are they helpful (you can look at point to plane ICP)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3b126",
   "metadata": {},
   "source": [
    "**Generating initial point clouds and visualizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78368304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "import open3d as o3d\n",
    "\n",
    "pcd = o3d.io.read_point_cloud(\"./bunny.pcd\")\n",
    "X = np.asarray(pcd.points)  \n",
    "X = X.T\n",
    "\n",
    "# Tuning the PCD a bit away form the origin \n",
    "X[0] = X[0] + 0.5\n",
    "X[1] = X[1] + 0.5\n",
    "X[2] = X[2] + 0.5\n",
    "\n",
    "# Downsampling 25 times\n",
    "X = X[:,::25]\n",
    "\n",
    "# Getting the second point cloud and shuffling all the points\n",
    "P = np.copy(X)\n",
    "P = P.T\n",
    "np.random.shuffle(P)\n",
    "P = P.T\n",
    "\n",
    "# Translating the second point cloud\n",
    "P[0,:] = P[0,:] + .25\n",
    "P[1,:] = P[1,:] + .25\n",
    "P[2,:] = P[2,:] + .25\n",
    "\n",
    "# Rotating the second point cloud\n",
    "theta1 = ( 180.0 / 360) * 2 * np.pi\n",
    "rot1 = np.array([[math.cos(theta1), -math.sin(theta1),0],\n",
    "                 [math.sin(theta1),  math.cos(theta1),0],\n",
    "               [0,0,1]])\n",
    "P1 = np.dot(rot1, P)\n",
    "\n",
    "# Visualzing the bunnies\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(X.T)\n",
    "pcd.paint_uniform_color([1, 0, 0])\n",
    "pcd1.points = o3d.utility.Vector3dVector(P1.T)\n",
    "pcd1.paint_uniform_color([0, 0, 1])\n",
    "o3d.visualization.draw_geometries([pcd,pcd1],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33977d05",
   "metadata": {},
   "source": [
    "**Nearest neighbor algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef7d81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # This is a brute force implementation for nearest neightbour\n",
    "# def nearest_neighbour(X,P):\n",
    "#     corres = []\n",
    "#     n = P.shape[1]    \n",
    "#     for ind in range(n):\n",
    "#         pc1 = P[:, ind]\n",
    "#         pc1 = pc1.reshape(3,1) \n",
    "#         dist = 9999999999999 \n",
    "#         idx = -1\n",
    "#         for i in range(n):           \n",
    "#             pc2 = X[:, i]\n",
    "#             pc2 = pc2.reshape(3,1)\n",
    "#             if np.sum(np.square(pc1-pc2)) < dist:\n",
    "#                 dist = np.sum(np.square(pc1-pc2))\n",
    "#                 idx = i\n",
    "#         corres.append(idx)\n",
    "#     return corres\n",
    "           \n",
    "# This is a KD trees implementation for nearest neighbour\n",
    "from sklearn.neighbors import KDTree\n",
    "tree = KDTree(X.T, leaf_size=2)   \n",
    "\n",
    "def nearest_neighbour(P):\n",
    "    corres = []\n",
    "    n = P.shape[1]    \n",
    "    for ind in range(n):\n",
    "        dist, ind = tree.query([P.T[ind,:]], k=1)\n",
    "        corres.append(ind[0][0])\n",
    "    return corres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036cfdcf",
   "metadata": {},
   "source": [
    "**Function to perform ICP alignment and get the aligned bunny**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICP(X, P):\n",
    "    total_iterations = 0\n",
    "    run  = True\n",
    "    last_error = np.float64(10000000000.000)\n",
    "    max_iterations = 200\n",
    "    n = X.shape[1]\n",
    "    dim = X.shape[0]\n",
    "    \n",
    "    while(run):\n",
    "        \n",
    "        total_iterations+=1\n",
    "        \n",
    "        # Getting nearest neighbors\n",
    "        cor = np.array(nearest_neighbour(P))\n",
    "        cor = cor.astype(int)\n",
    "        \n",
    "        # Getting errors with respect to corresponding nearest neighbors\n",
    "        X_copy = np.copy(X)\n",
    "        X_copy = X_copy[:, cor]\n",
    "        error = np.sqrt(np.sum(np.square(X_copy-P)) / X.shape[1])\n",
    "        \n",
    "        iteration_update = 'Iteration No.: ' + str(total_iterations) + ' '*(6-len(str(total_iterations)))\n",
    "        iteration_update += 'Current error: ' + str(error.round(5))\n",
    "\n",
    "        print(iteration_update)\n",
    "        last_error = error\n",
    "        \n",
    "        if(total_iterations == max_iterations):\n",
    "            run = False\n",
    "            print(\"Maximum iterations reached! Stopping!\")\n",
    "\n",
    "        if(error < 0.00001 and abs(last_error - error) < 0.00000001):\n",
    "            run = False\n",
    "            print(\"Aligned successfully!\")\n",
    "            \n",
    "        point_cloud1_mean = np.mean(X, axis = 1, keepdims = True)\n",
    "        point_cloud2_mean = np.mean(P, axis = 1, keepdims = True)\n",
    "\n",
    "        W = np.zeros((3,3))\n",
    "        \n",
    "        for ind in range(n):\n",
    "            point1 = X[:, cor[ind]]\n",
    "            point1 = point1.reshape(3,1)\n",
    "            arr1 = point1 - point_cloud1_mean\n",
    "            point2 = P[:, ind]\n",
    "            point2 = point2.reshape(3,1)\n",
    "            arr2 = point2 - point_cloud2_mean\n",
    "            W = W+np.dot(arr1,arr2.T)\n",
    "\n",
    "        W = W/n\n",
    "        U, S, Vt = np.linalg.svd(W)\n",
    "        R = np.dot(U,Vt)\n",
    "\n",
    "        if np.linalg.det(R) < 0:\n",
    "            Vt[dim - 1, :]*=-1\n",
    "            R = U @ Vt\n",
    "            \n",
    "        t = point_cloud1_mean - np.dot( R , point_cloud2_mean)\n",
    "        P = np.dot(R, P) + t\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67928f4e",
   "metadata": {},
   "source": [
    "**Calling ICP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09f519",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "P1_aligned = ICP(X,P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648aef8",
   "metadata": {},
   "source": [
    "**Visualizing the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(X.T)\n",
    "pcd.paint_uniform_color([1, 0, 0])\n",
    "pcd1.points = o3d.utility.Vector3dVector(P1_aligned.T)\n",
    "pcd1.paint_uniform_color([0, 0, 1])\n",
    "o3d.visualization.draw_geometries([pcd,pcd1],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb468d9",
   "metadata": {},
   "source": [
    "**Does ICP always give the correct alignment? Why or Why not?**  \n",
    "No, ICP without known correspondeces will not always give the correct alignment. If we choose nearest neighbours using distance, then in some transformations, we may repreatedly end up with incorrect matches. The algorithm will not converge in such cases. An example of such state is the image below, for which our code is unable to align point clouds successfully.\n",
    "\n",
    "<img src=\"P1.png\" alt=\"drawing\" width=\"600\"/>\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172794de",
   "metadata": {},
   "source": [
    "**What are other variants of ICP and why are they helpful (you can look at point to plane ICP)?**\n",
    "\n",
    "* **Different correspondence search method**: We have used closest point matching to get our correspondences. However, there are other variants as well such as normal shooting, and closest compatible point. In Point to Plane ICP variant, the error function to be minimized is not the sum of distance between corresponding pairs of points, but sum of the squared  distance  between  each source point and the tangent plane at its corresponding destination point. This is generally solved using the LM method. In this variant, each iteration in generally slower but the convergence rate is significantly better. Similarly we can use different correspondence search methods in different variants. It can be based on surface normals or computing other features for each point (features that capture shape around the point) and then look for matching features. This will give us better corresponces, that will help in the algorithm converging faster.\n",
    "\n",
    "* **Point subsets (from one or both point sets)**: In our code, we have done random downsampling in both point clouds, however other methods for downsampling are possible as well such as Feature-based sampling (which precomputes and chooses important points, making the corresponding search easier) and Normal-space sampling (which ensures that the samples have normals distributed as uniformly as possible). Less information loss is expected in such variants, compared to random downsampling, and hence higher chances of convergence (if we are using a feature based correspondence search method).\n",
    "\n",
    "* **Weighting the correspondences and rejecting outliers** - Once we have searched for corresponding pairs, we can assign weights to these pairs, based on the confidence values (low weight to pairs involving points with low confidence), or assign lower weights to points with higher pair distances. Or instead, we can directly reject pairs with point to point distance larger than a threshold. This will help us overcome noise due to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41752ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
