{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-1: Transformations and representations\n",
    "\n",
    "Team Name: Bhagwaan Bharose\n",
    "\n",
    "Roll number: 2019111019, 2019111026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "- Code must be written in Python in Jupyter Notebooks. We highly recommend using anaconda distribution or at the minimum, virtual environments for this assignment. See `Set Up` for detailed step-by-step instructions about the installation setup.\n",
    "- Save all your results in ```results/<question_number>/<sub_topic_number>/```\n",
    "- The **References** section provides you with important resources to solve the assignment.\n",
    "- For this assignment, you will be using Open3D extensively. Refer to [Open3D Documentation](http://www.open3d.org/docs/release/): you can use the in-built methods and **unless explicitly mentioned**, don't need to code from scratch for this assignment. \n",
    "- Make sure your code is modular since you may need to reuse parts for future assignments.\n",
    "- Answer the descriptive questions in your own words with context & clarity. Do not copy answers from online resources or lecture notes.\n",
    "- The **deadline** for this assignment is on 11/09/2021 at 11:55pm. Please note that there will be no extensions.\n",
    "- Plagiarism is **strictly prohibited**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "1. Make sure your code runs without any errors after reinitializing the kernel and removing all saved variables.\n",
    "2. After completing your code and saving your results, zip the folder with name as ``Team_<team_name>_MR2021_Assignment_<assignment_number>.zip``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up\n",
    "\n",
    "We highly recommend using anaconda distribution or at the minimum, virtual environments for this assignment. All assignments will be python based, hence familiarising yourself with Python is essential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Anaconda environment (Recommended)\n",
    "\n",
    "1. Install Anaconda or Miniconda from [here](https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html) depending on your requirements.\n",
    "2. Now simply run `conda env create -f environment.yml` in the current folder to create an environment `mr_assignment1` (`environment.yml` can be found in `misc/`).\n",
    "3. Activate it using `conda activate mr_assignment1`.\n",
    "\n",
    "## Setting up Virtual environment using venv\n",
    "\n",
    "You can also set up a virtual environment using venv\n",
    "\n",
    "1. Run `sudo apt-get install python3-venv` from command line.\n",
    "2. `python3 -m venv ~/virtual_env/mr_assignment1`. (you can set the environment path to anything)\n",
    "3. `source ~/virtual_env/mr_assignment1/bin/activate`\n",
    "4. `pip3 install -r requirements.txt` from the current folder (`requirements.txt` can be found in `misc/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting started with Open3D\n",
    "\n",
    "Open3D is an open-source library that deals with 3D data, such as point clouds, mesh. We'll be using Open3D frequently as we work with point clouds. Let's start with something simple:\n",
    "\n",
    "<img src=\"misc/bunny.jpg\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "1. Read the Stanford Bunny file (in `data/`) given to you and visualise it using Open3D.\n",
    "2. Convert the mesh to a point cloud and change the colour of points.\n",
    "3. Set a predefined viewing angle (using Open3D) for visualization and display the axes while plotting.\n",
    "4. Scale, Transform, and Rotate the rabbit (visualise after each step).\n",
    "5. Save the point cloud as bunny.pcd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Part 1 - Read the Stanford Bunny file (in data/) given to you and visualise it using Open3D.\n",
    "mesh = o3d.io.read_triangle_mesh(\"./data/bunny.ply\")\n",
    "o3d.visualization.draw_geometries([mesh])\n",
    "mesh.compute_vertex_normals()\n",
    "o3d.visualization.draw_geometries([mesh])\n",
    "\n",
    "# Part 2 - Convert the mesh to a point cloud and change the colour of points.\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = mesh.vertices\n",
    "pcd.colors = mesh.vertex_colors\n",
    "pcd.normals = mesh.vertex_normals\n",
    "o3d.visualization.draw_geometries([pcd])\n",
    "pcd.paint_uniform_color([1, 0, 0])\n",
    "o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "# Part 3 - Set a predefined viewing angle (using Open3D) for visualization and display the axes while plotting.\n",
    "FOR = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.025, origin=[0,0,0])\n",
    "o3d.visualization.draw_geometries([pcd, FOR], \n",
    "                                  zoom=1, \n",
    "                                  front=[-0.01, 0.01, 0], \n",
    "                                  lookat=[-0.01, 0.01, 0],\n",
    "                                  up=[0,1,0]\n",
    "                                 )\n",
    "\n",
    "# Part 4 - Scale, Transform, and Rotate the rabbit (visualise after each step).\n",
    "\n",
    "#Scale\n",
    "pcd.scale(1.5, center=pcd.get_center())\n",
    "o3d.visualization.draw_geometries([pcd, FOR], \n",
    "                                  zoom=1, \n",
    "                                  front=[-0.01, 0.01, 0], \n",
    "                                  lookat=[-0.01, 0.01, 0],\n",
    "                                  up=[0,1,0]\n",
    "                                 )\n",
    "# Transform\n",
    "T = np.eye(4)\n",
    "T[:3, :3] = pcd.get_rotation_matrix_from_xyz((0, np.pi / 3, np.pi / 2))\n",
    "T[0, 3] = 0.1\n",
    "T[1, 3] = 0.1\n",
    "pcd_t = copy.deepcopy(pcd).transform(T)\n",
    "pcd = copy.deepcopy(pcd_t)\n",
    "o3d.visualization.draw_geometries([pcd, FOR], \n",
    "                                  zoom=1, \n",
    "                                  front=[-0.01, 0.01, 0], \n",
    "                                  lookat=[-0.01, 0.01, 0],\n",
    "                                  up=[0,1,0]\n",
    "                                 )\n",
    "# Rotate\n",
    "R = pcd.get_rotation_matrix_from_xyz((np.pi/2, 0, 0))\n",
    "pcd.rotate(R, center=(0, 0, 0))\n",
    "o3d.visualization.draw_geometries([pcd, FOR])\n",
    "\n",
    "\n",
    "# Part 5 - Save the point cloud as bunny.pcd.\n",
    "o3d.io.write_point_cloud(\"./results/1/5/bunny.pcd\", pcd) # result is being \"./results/1/5/bunny.pcd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transformations and representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Euler angles\n",
    "1. Write a function that returns a rotation matrix given the angles $\\alpha$, $\\beta$, and $\\gamma$ in radians (X-Y-Z)\n",
    "\n",
    "2. Solve for angles using ```fsolve from scipy``` for three initializations of your choice and compare.\n",
    "$$M(\\alpha , \\beta ,\\gamma)=\\left[\\begin{array}{rrr}0.26200263 & -0.19674724 & 0.944799 \\\\0.21984631 & 0.96542533 & 0.14007684 \\\\\n",
    "    -0.93969262 & 0.17101007 & 0.29619813\\end{array}\\right] \n",
    "$$\n",
    "\n",
    "$$N(\\alpha , \\beta ,\\gamma)=\\left[\\begin{array}{rrr}0 & -0.173648178 &  0.984807753 \\\\0 & 0.984807753 & 0.173648178 \\\\\n",
    "    -1 & 0 & 0\\end{array}\\right] \n",
    "$$\n",
    "\n",
    "3. What is a Gimbal lock? \n",
    "\n",
    "4. Show an example where a Gimbal lock occurs and visualize the Gimbal lock on the given bunny point cloud. You have to show the above by **animation** (cube rotating along each axis one by one).\n",
    "    - *Hint: Use Open3D's non-blocking visualization and discretize the rotation to simulate the animation. For example, if you want to rotate by $30^{\\circ}$ around a particular axis, do in increments of $5^{\\circ}$ 6 times to make it look like an animation.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R =  [[ 0.35355339 -0.61237244  0.70710678]\n",
      " [ 0.35355339 -0.61237244 -0.70710678]\n",
      " [ 0.8660254   0.5         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.68587319,  0.61206381,  0.39364433],\n",
       "       [-0.59765976,  0.16515076,  0.78455595],\n",
       "       [ 0.41518764, -0.77337127,  0.47907839]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def rotation_matrix(theta):\n",
    "    R_x = np.array([[1, 0, 0],\n",
    "                    [0, math.cos(theta[0]), -math.sin(theta[0])],\n",
    "                    [0, math.sin(theta[0]), math.cos(theta[0])]\n",
    "                   ])\n",
    "    R_y = np.array([[math.cos(theta[1]), 0, math.sin(theta[1])],\n",
    "                    [0, 1, 0],\n",
    "                    [-math.sin(theta[1]), 0, math.cos(theta[1])]\n",
    "                    ])\n",
    "    R_z = np.array([[math.cos(theta[2]), -math.sin(theta[2]), 0],\n",
    "                    [math.sin(theta[2]), math.cos(theta[2]), 0],\n",
    "                    [0, 0, 1]\n",
    "                    ])\n",
    "    gg = np.dot(np.dot(R_x,R_y), R_z)\n",
    "    return gg.round(15) # Remove `.round(15)` to get exact calculation done by numpy \n",
    "\n",
    "#Set the angles here\n",
    "alpha=np.pi/2\n",
    "beta=np.pi/4\n",
    "gamma=np.pi/3\n",
    "\n",
    "R=rotation_matrix([alpha, beta, gamma])\n",
    "print(\"R = \", R)\n",
    "rotation_matrix([2.119, 2.737, 2.413])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, if A represents the fixed frame and B represents the rotating frame then the first rotation happens about X<sub>B</sub> by α, then about Y<sub>B</sub> by β, and finally about Z<sub>B</sub> by γ. In the above output, the rotation matrix is parameterized by X-Y-Z Euler angles will be indicated by the notation <sup>A</sup>R<sub>B</sub> <sub>XYZ</sub> (α, β, γ). \n",
    "Thinking of the rotations as descriptions of these frames, we can immediately write <sup>A</sup>R<sub>B</sub> = <sup>A</sup>R<sub>B1</sub> * <sup>B1</sup>R<sub>B2</sub> * <sup>B2</sup>R<sub>B3</sub>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.119 2.737 2.413]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "def rotation_equations(thetas):\n",
    "    \n",
    "    ca=np.cos(thetas[0])\n",
    "    cb=np.cos(thetas[1])\n",
    "    cc=np.cos(thetas[2])\n",
    "    sa=np.sin(thetas[0])\n",
    "    sb=np.sin(thetas[1])\n",
    "    sc=np.sin(thetas[2])\n",
    "\n",
    "    return [\n",
    "        -cb*sc - R[0][1],\n",
    "        -sa*sb*sc+ca*cc - R[1][1],\n",
    "        ca*sb*sc+sa*cc - R[2][1]\n",
    "    ]\n",
    "\n",
    "x = fsolve(rotation_equations, [0.1,0.1,0.1])\n",
    "print((x%np.pi).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Quaternions\n",
    "\n",
    "1. What makes Quaternions popular in graphics? \n",
    "2. Convert a rotation matrix to quaternion and vice versa. Do not use inbuilt libraries for this question.\n",
    "3. Perform matrix multiplication of two $\\mathcal{R}_{3 \\times 3}$ rotation matrices and perform the same transformation in the quaternion space. Verify if the final transformation obtained in both the cases are the same.\n",
    "4. Try to interpolate any 3D model (cube / bunny / not sphere obviously!!) between two rotation matrices and visualize!\n",
    "\n",
    "The above questions require you to **code your own functions** and **only verify** using inbuilt functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Exponential maps (Bonus)\n",
    "\n",
    "1. What is the idea behind exponential map representation of rotation matrices?\n",
    "2. Perform matrix exponentiation and obtain the rotation matrix to rotate a vector $P$ around $\\omega$ for $\\theta$ seconds.\n",
    "$$\n",
    "\\omega = \\begin{bmatrix}2 \\\\ 1 \\\\ 15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta = 4.1364\n",
    "$$\n",
    "\n",
    "3. Compute the logarithmic map (SO(3) to so(3)) of the rotation matrix to obtain the rotation vector and the angle of rotation\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.1 &  -0.9487 & 0.3 \\\\\n",
    "0.9487 & 0.  & -0.3162 \\\\\n",
    "0.3   &  0.3162  & 0.9 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "You can use inbuilt libraries **only to verify** your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Octomaps\n",
    "\n",
    "1. Why is an Octomap memory efficient?\n",
    "2. When do we update an Octomap and why?\n",
    "3. When would you likely use an octomap instead of a point cloud?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**  \n",
    "  \n",
    "1. An octomap uses an octree data structure to store the occupancy probabilities. Octree is a hierarchial data structure for spatial subdivisions in 3 dimensions. Each node in an octree represents the space contained in  a cubic volume, usually called a voxel. Each voxel can recursively be subdivided into 8 smaller voxels till the minimum size is reached. This approach is memory efficient compared to point cloud approach as instead of dividing the entire volume into individual cells, **we divide a cell further only if we have any conflict**. Hence, if we know for a huge volume that it is unoccupied, we may not further divide it. This helps us save memory, since in the case of point cloud, we would have stored the occupancy probability of each point cloud within that volume.  \n",
    "<br></br>  \n",
    "2. Octomap are **built incrementally**. As the robot moves around, we get occupancy probabilities from different camera locations (get new measurements) in a local map. We then first transform this data from local map to global space and then stitch the information by updating the octomap. The update is done using the recursive method (multiplying occupancy probabilities and multiplying non-occupancy probabilities obtained from different measurements and normalizing the results to get the occupancy and non-occupancy probabilities).   \n",
    "Octomap is updated because:  \n",
    "1) After each measurement, as we update the octomap, we progressively become more confident that a cell is occupied or unoccupied with more certainity (not talking about dynamic obstacles).  \n",
    "2) A robot can collect data only for a limited range (local space). Updating the octomap helps us to helps fit the data from local space to global map so as the robot moves around, we have a bigger picture (occupancy probabilities) of the entire global map.  \n",
    "<br></br>\n",
    "3. If we don't want sematic understanding of the scene (sensing and identifying the objects/obstacles in the scene) and **can work with the information whether a volume is occupied or unaoccpied**, then we can use octomaps (as it doesn't replicate the same scene in its understanding but only measures the occupancy probabilities of the voxels). Also we will use an octomap instead of a point cloud when we want the system to be **memory efficient and the access time is not a problem** for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Signed Distance Functions\n",
    "\n",
    "1. How do we determine object surfaces using SDF?\n",
    "2. How do we aggregate views from multiple cameras? (just a general overview is fine)\n",
    "3. Which preserves details better? Voxels or SDF? Why?\n",
    "4. What’s an advantage of SDF over a point cloud?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**  \n",
    "1. In case of SDF, instead of storing probability of occupancy of a voxel, here we try to represent the distance to the surface. For each voxel, we try estimating how far it is from the surface. After applying SDF to each voxel, if the sign is negative, the voxel lies outside the surface, if the value is zero, it lies on the surface and if it is positive then it lies inside the surface. So **if SDF for a voxel is 0, it means that it represents the object's surface**. Also, since we have the distance as well for each voxel, **after some linear calculation we can find out the surface of the object mathematically** (diagram below). \n",
    "<img src=\"H1.jpg\" alt=\"drawing\" width=\"600\"/>\n",
    "<br></br>  \n",
    "2. Similar to octomaps, after we have one measurement, we need to fuse the values from the next measurements, This is done using a weighted average of the measurements. For each voxel, we have a distance (D) and weight (W) with is initially 0. Now each measurement for a voxel will have a distance (d) and weight (w) associated to it. Now values for a voxel are updated as:  \n",
    "* $D=\\frac{DW+dw}{W+w}$   \n",
    "* W=W+w  \n",
    "Updating stategies can differ from implementation to implementation. As we keep on aggregating views from multiple cameras, W keeps on increasing and hence we will get more confident (since this means we have more measurements). So we can use the value of W as confidence.\n",
    "<br></br>  \n",
    "3. test  \n",
    "<br></br>  \n",
    "4. Advantages of SDF over point clouds are:\n",
    "* SDF gives us 2 values, weights and distance to the surface wheres point clouds only store occupancy probability.\n",
    "* Memory usage is high in point clouds, wheres high compression can be done in SDF as voxels nearer to the surface are stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Resources\n",
    "\n",
    "1. Gimbal locks and quaternions: https://youtu.be/YF5ZUlKxSgE\n",
    "2. Exponential map: \n",
    "    1. 3 Blue 1 Brown: https://youtu.be/O85OWBJ2ayo\n",
    "    2. Northwestern Robotics: https://youtu.be/v_KBHaG0mas\n",
    "3. Bunny ply is taken from: http://graphics.im.ntu.edu.tw/~robin/courses/cg03/model/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
