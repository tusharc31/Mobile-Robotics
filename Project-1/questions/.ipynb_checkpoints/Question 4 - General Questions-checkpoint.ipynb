{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mexican-confirmation",
   "metadata": {},
   "source": [
    "# Question 4: General Theory/Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-cleaners",
   "metadata": {},
   "source": [
    "_No need to be verbose, it's not fun for anyone_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-hindu",
   "metadata": {},
   "source": [
    "1. What part of SLAM did this project deal with? Why? What does the other part deal with and how would it generally work, given that you only have LIDAR scans, RGB video stream, and noisy pose data for a moving robot?\n",
    "\n",
    "\n",
    "2. Loop closures play an important role in reducing drift, how would you go about detecting these?\n",
    "\n",
    "\n",
    "3. Explain the structure of your Jacobian. Is the pose-graph fully connected? Why/Why not?\n",
    "\n",
    "\n",
    "4. With what you know now, how would you describe and differentiate the SLAM frontend and backend? Why do we need to optimise our poses/map in the first place - where does the noise come from/why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-bones",
   "metadata": {},
   "source": [
    "_Your Answer_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd64b7",
   "metadata": {},
   "source": [
    "1) Our project deals with the backend part of the SLAM as we deal with optimizing the poses based on the constraints given to us. The other part, that is the SLAM frontend deals with getting the constraints from the observations (this has been explained well in fourth part).\n",
    "\n",
    "2) Loop closure occurs when the robot comes back to a previously seen place. This time the robot can also generate constraints between present and previous poses. This can work through an ICP approach. We take previous lidar scans and align them with scan from present pose, and through the alignment, we can compute the relative transformation between the two poses. We can also use some vision based algorithms such as 5 point algorithm or 8 point algorithm if we are able to detect corresponding points between images from previous poses and current pose.\n",
    "\n",
    "3) pass\n",
    "\n",
    "4) Our project deals with (pose optimisation) SLAM backend but a real world system needs more. As the robot moves around, we need to turn our observations into constraints. This is called SLAM front end. This involves taking the raw sensor data and turning the sensor data into constraints. By this we relate the poses with each other and then they are optimized during SLAM backend. The front end part is basically the ICP algorithm that we did in assignment 3. ICP can be used generate the constraints (relative transformations between the poses). Taking these constraints and optimizing the graph by correcting the poses, which we did in this project is the SLAM backend. We need to optimize the poses, due to noise. Odometry/sensor/ICP measurements always have noise in them due to error in readings of the sensor and errors introduced by the algorithm. Hence, we try to correct our poses by taking excess constraints (that is, number of constraints is greater than the number of poses) and optimizing our poses based on the confidence we have on the readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0608fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
